{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Neural Networks Implementation from Scratch\n",
                "\n",
                "In this notebook, we implement a simple 2-layer Multi-Layer Perceptron (MLP) from scratch using only NumPy. We will focus on the core components: initialization, forward propagation, backpropagation, and parameter updates (Gradient Descent).\n",
                "\n",
                "## Network Architecture\n",
                "- **Input Layer**: 784 neurons (for 28x28 images)\n",
                "- **Hidden Layer**: 64 neurons with ReLU activation\n",
                "- **Output Layer**: 10 neurons with Softmax activation (for 10 classes)\n",
                "\n",
                "## Mathematical Equations\n",
                "### Forward Propagation\n",
                "1. $Z_1 = W_1 X + b_1$\n",
                "2. $A_1 = \\text{ReLU}(Z_1)$\n",
                "3. $Z_2 = W_2 A_1 + b_2$\n",
                "4. $A_2 = \\sigma(Z_2)$ (Softmax)\n",
                "\n",
                "### Backpropagation\n",
                "1. $dZ_2 = A_2 - Y$ (assuming cross-entropy loss)\n",
                "2. $dW_2 = \\frac{1}{m} dZ_2 A_1^T$\n",
                "3. $db_2 = \\frac{1}{m} \\sum dZ_2$\n",
                "4. $dZ_1 = W_2^T dZ_2 \\cdot \\text{ReLU}'(Z_1)$\n",
                "5. $dW_1 = \\frac{1}{m} dZ_1 X^T$\n",
                "6. $db_1 = \\frac{1}{m} \\sum dZ_1$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.datasets import fetch_openml\n",
                "from sklearn.model_selection import train_test_split\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Neural Network Class Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NeuralNetwork:\n",
                "    def __init__(self, input_size, hidden_size, output_size):\n",
                "        # Xavier/Kaiming-like initialization\n",
                "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
                "        self.b1 = np.zeros((hidden_size, 1))\n",
                "        self.W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
                "        self.b2 = np.zeros((output_size, 1))\n",
                "\n",
                "    def relu(self, Z):\n",
                "        return np.maximum(0, Z)\n",
                "\n",
                "    def softmax(self, Z):\n",
                "        exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
                "        return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
                "\n",
                "    def relu_derivative(self, Z):\n",
                "        return Z > 0\n",
                "\n",
                "    def forward(self, X):\n",
                "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
                "        self.A1 = self.relu(self.Z1)\n",
                "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
                "        self.A2 = self.softmax(self.Z2)\n",
                "        return self.A2\n",
                "\n",
                "    def backward(self, X, Y, A2, m):\n",
                "        dZ2 = A2 - Y\n",
                "        dW2 = (1 / m) * np.dot(dZ2, self.A1.T)\n",
                "        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
                "        \n",
                "        dZ1 = np.dot(self.W2.T, dZ2) * self.relu_derivative(self.Z1)\n",
                "        dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
                "        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
                "        \n",
                "        return dW1, db1, dW2, db2\n",
                "\n",
                "    def update_params(self, dW1, db1, dW2, db2, alpha):\n",
                "        self.W1 -= alpha * dW1\n",
                "        self.b1 -= alpha * db1\n",
                "        self.W2 -= alpha * dW2\n",
                "        self.b2 -= alpha * db2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training and Evaluation Utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def one_hot(Y, classes):\n",
                "    one_hot_Y = np.zeros((classes, Y.size))\n",
                "    one_hot_Y[Y.astype(int), np.arange(Y.size)] = 1\n",
                "    return one_hot_Y\n",
                "\n",
                "def get_predictions(A2):\n",
                "    return np.argmax(A2, axis=0)\n",
                "\n",
                "def get_accuracy(predictions, Y):\n",
                "    return np.sum(predictions == Y) / Y.size\n",
                "\n",
                "def train(X, Y, iterations, alpha, hidden_size):\n",
                "    input_size = X.shape[0]\n",
                "    output_size = len(np.unique(Y))\n",
                "    nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
                "    m = X.shape[1]\n",
                "    Y_one_hot = one_hot(Y, output_size)\n",
                "    \n",
                "    for i in range(iterations):\n",
                "        A2 = nn.forward(X)\n",
                "        dW1, db1, dW2, db2 = nn.backward(X, Y_one_hot, A2, m)\n",
                "        nn.update_params(dW1, db1, dW2, db2, alpha)\n",
                "        if i % 100 == 0:\n",
                "            print(\"Iteration: \", i)\n",
                "            predictions = get_predictions(A2)\n",
                "            print(get_accuracy(predictions, Y))\n",
                "    return nn"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}